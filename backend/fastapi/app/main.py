from fastapi import FastAPI, HTTPException, Header, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
import os
from dotenv import load_dotenv
import uvicorn
import httpx
import json
from typing import AsyncGenerator, Optional
from .django_client import django_client
from .redis_client import get_redis, get_image_queue, is_redis_available

# Load environment variables
load_dotenv()

app = FastAPI(
    title="EduChat FastAPI",
    description="OpenAI streaming & image generation service for EduChat",
    version="1.0.0"
)

# CORS (configure from env, default to *)
allow_origins = os.getenv("CORS_ALLOW_ORIGINS", "*")
app.add_middleware(
    CORSMiddleware,
    allow_origins=[o.strip() for o in allow_origins.split(",") if o.strip()] if allow_origins != "*" else ["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# OpenAI Configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
DJANGO_BASE_URL = os.getenv("DJANGO_BASE_URL", "")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o")
DALLE_MODEL = os.getenv("DALLE_MODEL", "dall-e-3")

if not OPENAI_API_KEY:
    raise ValueError("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤! .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")


@app.get("/")
def root():
    return {
        "service": "fastapi",
        "status": "ok",
        "endpoints": [
            "/health",
            "/readiness",
            "/chat/stream",
            "/image/generate"
        ]
    }

@app.get("/health")
def health():
    """í—¬ìŠ¤ ì²´í¬"""
    redis_status = "connected" if is_redis_available() else "disabled"
    return {
        "ok": True,
        "redis": redis_status
    }

@app.get("/readiness")
def readiness():
    """ì¤€ë¹„ ìƒíƒœ í™•ì¸"""
    redis_ready = False
    if is_redis_available():
        try:
            redis_client = get_redis()
            redis_client.ping()
            redis_ready = True
        except:
            redis_ready = False
    
    return {
        "ready": True,
        "redis": redis_ready,
        "openai": bool(OPENAI_API_KEY),
        "django": bool(DJANGO_BASE_URL)
    }


# ==================== Chat Streaming (OpenAI) ====================

import random
import re

# ì´ëª¨ì§€ í’€ - ìƒí™©ë³„ë¡œ ë¶„ë¥˜
EMOJI_POOLS = {
    "happy": ["ğŸ˜Š", "ğŸ˜ƒ", "ğŸ˜„", "ğŸ˜", "ğŸ™‚", "ğŸ˜Œ", "ğŸ¤—", "ğŸ¥°", "âœ¨", "ğŸ’«", "ğŸŒŸ"],
    "thinking": ["ğŸ¤”", "ğŸ§", "ğŸ’­", "ğŸ¤¨", "ğŸ˜¯", "ğŸ˜®"],
    "excited": ["ğŸ¤©", "ğŸ˜", "ğŸ‰", "ğŸŠ", "â­", "ğŸ’«", "âœ¨", "ğŸŒŸ"],
    "love": ["ğŸ˜", "ğŸ¥°", "ğŸ˜˜", "ğŸ’•", "ğŸ’–", "ğŸ’—", "ğŸ’"],
    "surprised": ["ğŸ˜®", "ğŸ˜¯", "ğŸ˜²", "ğŸ¤¯", "ğŸ˜³"],
    "sad": ["ğŸ˜”", "ğŸ˜•", "ğŸ™", "â˜¹ï¸", "ğŸ˜¢", "ğŸ˜¥"],
    "neutral": ["ğŸ˜", "ğŸ˜‘", "ğŸ˜¶", "ğŸ™‚"],
    "playful": ["ğŸ˜", "ğŸ˜œ", "ğŸ˜", "ğŸ˜›", "ğŸ¤ª", "ğŸ˜‹"],
}

# ë°˜ë³µë˜ëŠ” ì´ëª¨ì§€ ì¶”ì  (ìµœê·¼ 5ê°œ)
recent_emojis = []

def diversify_emoji(text: str) -> str:
    """
    ë°˜ë³µë˜ëŠ” ì´ëª¨ì§€ë¥¼ ë‹¤ì–‘í•œ ì´ëª¨ì§€ë¡œ êµì²´
    """
    global recent_emojis
    
    # ì´ëª¨ì§€ íŒ¨í„´ ì°¾ê¸° (ìœ ë‹ˆì½”ë“œ ì´ëª¨ì§€)
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # í‘œì •
        "\U0001F300-\U0001F5FF"  # ê¸°í˜¸ & í”½í† ê·¸ë¨
        "\U0001F680-\U0001F6FF"  # êµí†µ & ì§€ë„
        "\U0001F1E0-\U0001F1FF"  # êµ­ê¸°
        "\U00002600-\U000027BF"  # ê¸°íƒ€ ê¸°í˜¸
        "\U0001F900-\U0001F9FF"  # ì¶”ê°€ ì´ëª¨ì§€
        "\U0001FA70-\U0001FAFF"  # í™•ì¥ ì´ëª¨ì§€
        "\U00002702-\U000027B0"
        "\U000024C2-\U0001F251"
        "]+",
        flags=re.UNICODE
    )
    
    def replace_emoji(match):
        global recent_emojis
        emoji = match.group(0)
        
        # ì´ë¯¸ ìµœê·¼ì— ì‚¬ìš©ëœ ì´ëª¨ì§€ë©´ êµì²´
        if emoji in recent_emojis[-5:]:
            # ì£¼ë³€ í…ìŠ¤íŠ¸ë¡œ ê°ì • íŒë‹¨
            context = text[max(0, match.start()-20):match.end()+20].lower()
            
            # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í’€ ì„ íƒ
            if any(word in context for word in ["ìƒê°", "ê³ ë¯¼", "ìŒ", "ì–´ë–»"]):
                pool = EMOJI_POOLS["thinking"]
            elif any(word in context for word in ["ê¸°ì˜", "ì¢‹", "í™˜", "ì‹ ë‚˜", "ì§±", "ìµœê³ "]):
                pool = EMOJI_POOLS["happy"]
            elif any(word in context for word in ["ì™€", "ëŒ€ë‹¨", "ë©‹ì§€", "ë†€ë¼", "ì •ë§"]):
                pool = EMOJI_POOLS["excited"]
            elif any(word in context for word in ["ì‚¬ë‘", "ì¢‹ì•„", "ê·€ì—¬", "ì˜ˆì˜"]):
                pool = EMOJI_POOLS["love"]
            elif any(word in context for word in ["ìŠ¬í”„", "ì•„ì‰½", "ì•ˆíƒ€", "ê±±ì •"]):
                pool = EMOJI_POOLS["sad"]
            elif any(word in context for word in ["ì¬ë¯¸", "ì¥ë‚œ", "ì›ƒ", "í•˜í•˜"]):
                pool = EMOJI_POOLS["playful"]
            else:
                # ê¸°ë³¸ì ìœ¼ë¡œ happy í’€ ì‚¬ìš©
                pool = EMOJI_POOLS["happy"]
            
            # ìµœê·¼ ì‚¬ìš©í•˜ì§€ ì•Šì€ ì´ëª¨ì§€ ì„ íƒ
            available = [e for e in pool if e not in recent_emojis[-5:]]
            if not available:
                available = pool
            
            new_emoji = random.choice(available)
            recent_emojis.append(new_emoji)
            if len(recent_emojis) > 10:
                recent_emojis.pop(0)
            
            return new_emoji
        else:
            # ìƒˆë¡œìš´ ì´ëª¨ì§€ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
            recent_emojis.append(emoji)
            if len(recent_emojis) > 10:
                recent_emojis.pop(0)
            return emoji
    
    return emoji_pattern.sub(replace_emoji, text)


async def stream_chat_response(
    messages: list,
    system_prompt: str,
    temperature: float = 0.7,
    max_tokens: int = 2000,
) -> AsyncGenerator[str, None]:
    """
    Stream chat response from OpenAI API
    """
    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json",
    }

    payload = {
        "model": OPENAI_MODEL,
        "messages": [
            {"role": "system", "content": system_prompt},
            *messages
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
        "stream": True,
    }

    try:
        async with httpx.AsyncClient(timeout=None) as client:
            async with client.stream(
                "POST",
                f"{OPENAI_BASE_URL}/chat/completions",
                json=payload,
                headers=headers
            ) as response:
                if response.status_code != 200:
                    error_text = await response.aread()
                    error_message = error_text.decode('utf-8') if error_text else 'Unknown error'
                    yield f"data: {json.dumps({'error': f'OpenAI API error: {response.status_code} - {error_message}'})}\n\n"
                    return

                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data_str = line[6:]  # Remove "data: " prefix
                        if data_str == "[DONE]":
                            yield f"data: {json.dumps({'done': True})}\n\n"
                            break

                        try:
                            data = json.loads(data_str)
                            if "choices" in data and len(data["choices"]) > 0:
                                delta = data["choices"][0].get("delta", {})
                                if "content" in delta:
                                    # ì´ëª¨ì§€ ë‹¤ì–‘í™” ì ìš©
                                    content = diversify_emoji(delta['content'])
                                    yield f"data: {json.dumps({'content': content, 'done': False})}\n\n"
                        except json.JSONDecodeError:
                            continue

    except httpx.RequestError as e:
        yield f"data: {json.dumps({'error': f'Request error: {str(e)}'})}\n\n"
    except Exception as e:
        yield f"data: {json.dumps({'error': f'Unexpected error: {str(e)}'})}\n\n"


from pydantic import BaseModel, Field


class ChatStreamRequest(BaseModel):
    """Chat streaming request"""
    conversation_id: int = Field(..., description="Conversation ID from Django")
    character_id: int = Field(..., description="Character ID from Django")
    user_message: str = Field(..., description="User's message")
    user_token: str = Field(default="", description="User's JWT token for Django API")
    messages: list = Field(default=[], description="Previous message history [{'role': 'user|assistant', 'content': '...'}]")
    temperature: float = Field(default=0.7, ge=0.0, le=2.0)
    max_tokens: int = Field(default=2000, ge=100, le=4000)
    save_to_db: bool = Field(default=True, description="Save messages to Django DB")


class ImageGenerationRequest(BaseModel):
    """Image generation request"""
    user_token: str = Field(default="", description="User JWT token from frontend")
    prompt: str = Field(..., description="Image prompt")
    size: str = Field(default="1024x1024", description="Image size (1024x1024, 1024x1792, 1792x1024)")
    quality: str = Field(default="standard", description="Quality (standard or hd)")
    save_to_db: bool = Field(default=True, description="Save to Django DB")


@app.post("/chat/stream")
async def chat_stream(request: ChatStreamRequest):
    """
    Stream chat response from OpenAI

    1. Fetch character system prompt from Django API
    2. Save user message to Django DB
    3. Stream response from OpenAI
    4. Save assistant response to Django DB
    5. Return SSE stream
    """
    try:
        # 1. Fetch character system prompt from Django
        character_data = await django_client.get_character(request.character_id)
        if not character_data:
            raise HTTPException(
                status_code=404,
                detail=f"Character not found: {request.character_id}"
            )
        
        system_prompt = character_data.get("system_prompt", "You are a helpful assistant.")
        temperature = character_data.get("creativity", request.temperature)
        
        # 2. Save user message to DB (optional)
        if request.save_to_db:
            await django_client.save_message(
                conversation_id=request.conversation_id,
                role="user",
                content=request.user_message,
                user_token=request.user_token,
                model_version=OPENAI_MODEL,
            )
        
        # 3. Prepare messages for OpenAI
        all_messages = request.messages + [{"role": "user", "content": request.user_message}]
        
        # 4. Stream response and collect for saving
        collected_response = []
        
        async def stream_and_collect():
            """Stream from OpenAI and collect response"""
            async for chunk in stream_chat_response(
                messages=all_messages,
                system_prompt=system_prompt,
                temperature=temperature,
                max_tokens=request.max_tokens,
            ):
                # Parse SSE chunk
                if chunk.startswith("data: "):
                    try:
                        data = json.loads(chunk[6:])
                        if data.get("content"):
                            collected_response.append(data["content"])
                    except:
                        pass
                
                yield chunk
            
            # 5. Save assistant response to DB after streaming completes
            if request.save_to_db and collected_response:
                full_response = "".join(collected_response)
                await django_client.save_message(
                    conversation_id=request.conversation_id,
                    role="assistant",
                    content=full_response,
                    user_token=request.user_token,
                    token_usage=len(full_response.split()),  # ê°„ë‹¨í•œ í† í° ì¶”ì •
                    model_version=OPENAI_MODEL,
                )
        
        # Return SSE stream
        return StreamingResponse(
            stream_and_collect(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "X-Accel-Buffering": "no",
            }
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ==================== Image Generation (DALL-E) ====================

@app.post("/image/generate")
async def generate_image(request: ImageGenerationRequest):
    """
    Generate image using DALL-E 3
    
    Redis ì‚¬ìš© ì‹œ: ë¹„ë™ê¸° ì‘ì—… íì— ì¶”ê°€ (ê¶Œì¥)
    Redis ë¯¸ì‚¬ìš© ì‹œ: ë™ê¸° ì²˜ë¦¬ (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
    
    Returns: {
        "job_id": int,
        "task_id": str (Redis ì‚¬ìš© ì‹œ),
        "status": "queued" | "processing" | "completed",
        "url": "image_url" (ì™„ë£Œ ì‹œ),
        "message": "ìƒíƒœ ë©”ì‹œì§€"
    }
    """
    job_id = None
    image_queue = get_image_queue()
    
    try:
        # 1. Create generation job in Django (status: pending)
        if request.save_to_db and request.user_token:
            job_data = await django_client.create_generation_job(
                user_token=request.user_token,
                job_type="image",
                input_data={
                    "prompt": request.prompt,
                    "size": request.size,
                    "quality": request.quality,
                    "model": DALLE_MODEL,
                }
            )
            if job_data:
                job_id = job_data.get("id")
        elif request.save_to_db:
            print("[FastAPI] Warning: save_to_db=True but no user_token provided")
        
        # 2. Redis í ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°
        if image_queue:
            # Redis íì— ì‘ì—… ì¶”ê°€ (ë¹„ë™ê¸°)
            from .tasks import generate_image_task
            
            job = image_queue.enqueue(
                generate_image_task,
                prompt=request.prompt,
                size=request.size,
                quality=request.quality,
                job_id=str(job_id) if job_id else None,  # Convert int to string
                user_token=request.user_token,
                job_timeout='10m',  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
            )
            
            # Jobì„ processing ìƒíƒœë¡œ ì—…ë°ì´íŠ¸ (Redis íì— ì¶”ê°€ë¨)
            if job_id and request.save_to_db and request.user_token:
                await django_client.update_generation_job(
                    job_id=job_id,
                    status="processing",
                    user_token=request.user_token
                )

            return {
                "job_id": job_id,
                "task_id": job.id,
                "status": "processing",
                "message": "Image generation processing. Check status with /image/status/{task_id}",
                "check_url": f"/image/status/{job.id}",
                "success": True
            }
        
        else:
            # Redis ì—†ìŒ: ë™ê¸° ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)
            print("[FastAPI] Redis not available - processing synchronously")
            
            # processing ìƒíƒœë¡œ ë³€ê²½
            if job_id and request.save_to_db and request.user_token:
                await django_client.update_generation_job(
                    job_id=job_id,
                    status="processing",
                    user_token=request.user_token
                )
            
            # 2. Call DALL-E API
            headers = {
                "Authorization": f"Bearer {OPENAI_API_KEY}",
                "Content-Type": "application/json",
            }

            payload = {
                "model": DALLE_MODEL,
                "prompt": request.prompt,
                "n": 1,
                "size": request.size,
                "quality": request.quality,
            }

            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{OPENAI_BASE_URL}/images/generations",
                    json=payload,
                    headers=headers,
                    timeout=60.0
                )

                if response.status_code != 200:
                    error_data = response.json()
                    error_message = error_data.get('error', {}).get('message', 'Unknown error')
                    
                    # Update job status to failed
                    if job_id and request.save_to_db and request.user_token:
                        await django_client.update_generation_job(
                            job_id=job_id,
                            status="failed",
                            error_message=error_message,
                            user_token=request.user_token
                        )
                    
                    raise HTTPException(
                        status_code=response.status_code,
                        detail=f"DALL-E API error: {error_message}"
                    )

                result = response.json()
                image_url = result["data"][0]["url"]
                revised_prompt = result["data"][0].get("revised_prompt", request.prompt)
                
                # 3. Update job status to completed
                if job_id and request.save_to_db and request.user_token:
                    await django_client.update_generation_job(
                        job_id=job_id,
                        status="completed",
                        result_data={
                            "url": image_url,
                            "revised_prompt": revised_prompt,
                            "model": DALLE_MODEL,
                            "size": request.size,
                            "quality": request.quality,
                        },
                        user_token=request.user_token
                    )
                
                # 4. Return result
                return {
                    "job_id": job_id,
                    "url": image_url,
                    "revised_prompt": revised_prompt,
                    "model": DALLE_MODEL,
                    "success": True,
                    "status": "completed"
                }

    except HTTPException:
        raise
    except Exception as e:
        # Update job status to failed
        if job_id and request.save_to_db and request.user_token:
            await django_client.update_generation_job(
                job_id=job_id,
                status="failed",
                error_message=str(e),
                user_token=request.user_token
            )
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/image/status/{task_id}")
async def get_image_status(task_id: str):
    """
    Redis íì— ìˆëŠ” ì´ë¯¸ì§€ ìƒì„± ì‘ì—… ìƒíƒœ í™•ì¸
    
    Returns: {
        "task_id": str,
        "status": "queued" | "started" | "finished" | "failed",
        "result": dict (ì™„ë£Œ ì‹œ),
        "error": str (ì‹¤íŒ¨ ì‹œ)
    }
    """
    from rq.job import Job
    
    redis_client = get_redis()
    if not redis_client:
        raise HTTPException(
            status_code=503,
            detail="Redis not configured - status checking not available"
        )
    
    try:
        job = Job.fetch(task_id, connection=redis_client)
        
        response = {
            "task_id": task_id,
            "status": job.get_status(),
            "created_at": job.created_at.isoformat() if job.created_at else None,
            "started_at": job.started_at.isoformat() if job.started_at else None,
            "ended_at": job.ended_at.isoformat() if job.ended_at else None,
        }
        
        if job.is_finished:
            response["result"] = job.result
        elif job.is_failed:
            response["error"] = str(job.exc_info) if job.exc_info else "Unknown error"
        
        return response
        
    except Exception as e:
        raise HTTPException(
            status_code=404,
            detail=f"Job not found: {str(e)}"
        )


if __name__ == "__main__":
    import socket

    host = os.getenv("FASTAPI_HOST", "0.0.0.0")
    port_str = os.getenv("FASTAPI_PORT", "8080")
    port = int(port_str) if port_str.isdigit() else 8080

    # í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” reload=False
    is_production = os.getenv("ENVIRONMENT", "development").lower() == "production"
    reload = not is_production

    # í¬íŠ¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ ë° ìë™ ëŒ€ì²´ (ê°œë°œ í™˜ê²½ì—ì„œë§Œ)
    def find_available_port(start_port, max_attempts=10):
        for attempt in range(max_attempts):
            test_port = start_port + attempt
            try:
                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                    s.bind((host, test_port))
                    return test_port
            except OSError:
                continue
        raise RuntimeError(f"Could not find available port in range {start_port}-{start_port+max_attempts-1}")

    try:
        if is_production:
            # í”„ë¡œë•ì…˜: í¬íŠ¸ ìë™ ê°ì§€ ì—†ìŒ
            print(f"ğŸš€ Starting FastAPI (Production) on http://{host}:{port}")
            uvicorn.run("app.main:app", host=host, port=port, reload=False)
        else:
            # ê°œë°œ: í¬íŠ¸ ìë™ ê°ì§€
            available_port = find_available_port(port)
            if available_port != port:
                print(f"âš ï¸  Port {port} is in use. Using port {available_port} instead.")
            print(f"ğŸš€ Starting FastAPI (Development) on http://{host}:{available_port}")
            uvicorn.run("app.main:app", host=host, port=available_port, reload=True)
    except RuntimeError as e:
        print(f"âŒ Error: {e}")
        print("ğŸ’¡ Try specifying a different port: FASTAPI_PORT=8001 python -m app.main")